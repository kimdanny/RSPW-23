{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34034f63",
   "metadata": {},
   "source": [
    "# Day 3 - Machine Learning\n",
    "## Data Preprocessing\n",
    "\n",
    "### After this notebook you will be able to ...\n",
    "- load various datasets from sklearn\n",
    "- know the importance of data scaling and preprocessing\n",
    "- know that the sensitivity various between learning algorithms\n",
    "- split the dataset into train and test set\n",
    "- know what will happen if you apply different preprocessing steps to train and test set\n",
    "- know how to encode categorical features to ordinal representation and how it affects the model performance\n",
    "- know how to deal with missing data\n",
    "\n",
    "### Acknowledgements\n",
    "- https://github.com/kimdanny/COMP0189-practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f1108",
   "metadata": {},
   "source": [
    "## Introduction to Scikit-learn\n",
    "\n",
    "Why do we use sklearn??\n",
    "\n",
    "1. Example Datasets\n",
    "    - sklearn.datasets : Provides example datasets\n",
    "\n",
    "2. Feature Engineering  \n",
    "    - sklearn.preprocessing : Variable functions as to data preprocessing\n",
    "    - sklearn.feature_selection : Help selecting primary components in datasets\n",
    "    - sklearn.feature_extraction : Vectorised feature extraction\n",
    "    - sklearn.decomposition : Algorithms regarding Dimensionality Reduction\n",
    "\n",
    "3. Data split and Parameter Tuning  \n",
    "    - sklearn.model_selection : 'Train Test Split' for cross validation, Parameter tuning with GridSearch\n",
    "\n",
    "4. Evaluation  \n",
    "    - sklearn.metrics : accuracy score, ROC curve, F1 score, etc.\n",
    "\n",
    "5. ML Algorithms\n",
    "    - sklearn.ensemble : Ensemble, etc.\n",
    "    - sklearn.linear_model : Linear Regression, Logistic Regression, etc.\n",
    "    - sklearn.naive_bayes : Gaussian Naive Bayes classification, etc.\n",
    "    - sklearn.neighbors : Nearest Centroid classification, etc.\n",
    "    - sklearn.svm : Support Vector Machine\n",
    "    - sklearn.tree : DecisionTreeClassifier, etc.\n",
    "    - sklearn.cluster : Clustering (Unsupervised Learning)\n",
    "\n",
    "6. Utilities  \n",
    "    - sklearn.pipeline: pipeline of (feature engineering -> ML Algorithms -> Prediction)\n",
    "\n",
    "7. Train and Predict  \n",
    "    - fit()\n",
    "    - predict()\n",
    "\n",
    "8. and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0eca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949cac4",
   "metadata": {},
   "source": [
    "**Wine Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17250624",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_X = wine.data\n",
    "wine_labels = wine.target\n",
    "wine_feature_names = wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d26d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_X, columns=wine_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_wine(X, labels=None, column_indices=(0,1), set_labels=False):\n",
    "    \"\"\"\n",
    "    @param: X        --> Data\n",
    "    @param: lables   --> Default is set to None, but if you've got your result of labels from clustering, \n",
    "                         you can input according labels in a list format.\n",
    "    @param: column_indices --> column indices of dataset X to be selected for plotting.\n",
    "                                 two-element tuple if you want 2D graph,\n",
    "                                 three-element tuple if you want 3D graph.\n",
    "    \"\"\"\n",
    "    assert type(column_indices) is tuple\n",
    "    \n",
    "    if len(column_indices)==2:  # 2D\n",
    "        first_col, second_col = column_indices[0], column_indices[1]\n",
    "        \n",
    "        if set_labels:\n",
    "            plt.xlabel(wine_feature_names[first_col])\n",
    "            plt.ylabel(wine_feature_names[second_col])\n",
    "            \n",
    "        plt.scatter(X[:, first_col], X[:, second_col], c=labels)\n",
    "        \n",
    "    elif len(column_indices)==3:  # 3D\n",
    "        first_col, second_col, third_col = column_indices[0], column_indices[1], column_indices[2]\n",
    "        fig = plt.figure()\n",
    "        plt.clf()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        plt.cla()\n",
    "        \n",
    "        if set_labels:\n",
    "            ax.set_xlabel(wine_feature_names[first_col])\n",
    "            ax.set_ylabel(wine_feature_names[second_col])\n",
    "            ax.set_zlabel(wine_feature_names[third_col])\n",
    "\n",
    "        ax.scatter(X[:, first_col], X[:, second_col], X[:, third_col], c=labels)\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(\"Your dimension should be either set to \\\"2d\\\" or \\\"3d\\\"\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36356fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10), set_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out different col_in_X and get some feeling of how the data is shaped.\n",
    "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10, 12), set_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a2680",
   "metadata": {},
   "source": [
    "We will closely look into details of many functions in scikit-learn (fit, predict, PCA, metrics, etc.) in the following practicals as we learn more in lectures.  \n",
    "For now, it is good to be familiar with datasets and the main takeaways we demonstrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6973e",
   "metadata": {},
   "source": [
    "## Exercise 1: Impact of feature scaling in ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dede2",
   "metadata": {},
   "source": [
    "Normalization scales each input variable separately to the range 0-1.  \n",
    "Standardization scales each input variable separately by subtracting the mean (centering) and dividing each of them by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18994d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b1bba",
   "metadata": {},
   "source": [
    "#### Examaple usage of sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "unscaled_data = np.asarray([[100, 0.001],\n",
    " [8, 0.05],\n",
    " [50, 0.005],\n",
    " [88, 0.07],\n",
    " [4, 0.1]])\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled_data = scaler.fit_transform(unscaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(unscaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaled_data, unscaled_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24e99f",
   "metadata": {},
   "source": [
    "**Questions**  \n",
    "- Try using different methods, such as MinMaxScaler and Normalisation. Do you see the difference in the histogram?\n",
    "- Experiment the effects of different feature scaling methods on one ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fa5bd",
   "metadata": {},
   "source": [
    "### Scaling Vs. Unscaling the Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5504b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "# We are using the wind dataset again\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "# TODO: Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "unscaled_X_train = X_train\n",
    "unscaled_X_test = X_test\n",
    "\n",
    "# scale data\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_model = KNeighborsClassifier()\n",
    "scaled_model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_model.fit(unscaled_X_train, y_train)\n",
    "scaled_model.fit(scaled_X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_y_hat = unscaled_model.predict(unscaled_X_test)\n",
    "scaled_y_hat = scaled_model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b066667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get accuracy scores of both unscaled and scaled model with test set\n",
    "unscaled_acc = accuracy_score(None, None)\n",
    "scaled_acc = accuracy_score(None, None)\n",
    "unscaled_acc, scaled_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b715b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Impact of different preprocessing strategy in train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf828b",
   "metadata": {},
   "source": [
    "Do you see the difference in accuracy?  \n",
    "**Question**  \n",
    "Now, notice that I also scaled the test set.   \n",
    "Using the same code, see what happens if you don't scale the test data and predict based on the unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try using the same test data for both unscaled and scaled model\n",
    "unscaled_y_hat = unscaled_model.predict(None)\n",
    "scaled_y_hat = scaled_model.predict(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_acc = accuracy_score(y_test, unscaled_y_hat)\n",
    "scaled_acc = accuracy_score(y_test, scaled_y_hat)\n",
    "unscaled_acc, scaled_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8567690",
   "metadata": {},
   "source": [
    "## sklearn.pipeline.make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e93b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_make_pipeline(pca_enable=False):\n",
    "    # Fit to data and predict using pipeline\n",
    "    if pca_enable:\n",
    "        unscaled_clf = make_pipeline(PCA(n_components=2), KNeighborsClassifier())\n",
    "    else:\n",
    "        unscaled_clf = make_pipeline(KNeighborsClassifier())\n",
    "    unscaled_clf.fit(X_train, y_train)\n",
    "    pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "    # Fit to data and predict using pipeline\n",
    "    if pca_enable:\n",
    "        std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), KNeighborsClassifier())\n",
    "    else:\n",
    "        std_clf = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "    std_clf.fit(X_train, y_train)\n",
    "    pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "    # Show prediction accuracies in scaled and unscaled data.\n",
    "    print(\"\\nPrediction accuracy for the normal test dataset\")\n",
    "    print(f\"{accuracy_score(y_test, pred_test):.2%}\\n\")\n",
    "\n",
    "    print(\"\\nPrediction accuracy for the standardized test dataset\")\n",
    "    print(f\"{accuracy_score(y_test, pred_test_std):.2%}\\n\")\n",
    "\n",
    "    # Extract PCA from pipeline3\n",
    "    # print(unscaled_clf.named_steps)\n",
    "    # {'pca': PCA(n_components=2), 'gaussiannb': GaussianNB()}\n",
    "    # print(std_clf.named_steps)\n",
    "    # {'standardscaler': StandardScaler(), 'pca': PCA(n_components=2), 'gaussiannb': GaussianNB()}\n",
    "\n",
    "    try:\n",
    "        pca = unscaled_clf.named_steps[\"pca\"]\n",
    "        pca_std = std_clf.named_steps[\"pca\"]\n",
    "\n",
    "        # Show first principal components\n",
    "        print(f\"\\nPC 1 without scaling:\\n{pca.components_[0]}\")\n",
    "        print(f\"\\nPC 1 with scaling:\\n{pca_std.components_[0]}\")\n",
    "\n",
    "        # Use PCA without and with scale on X_train data for visualization.\n",
    "        X_train_transformed = pca.transform(X_train)\n",
    "\n",
    "        scaler = std_clf.named_steps[\"standardscaler\"]\n",
    "        scaled_X_train = scaler.transform(X_train)\n",
    "        X_train_std_transformed = pca_std.transform(scaled_X_train)\n",
    "\n",
    "        # visualize standardized vs. untouched dataset with PCA performed\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "\n",
    "        target_classes = range(0, 3)\n",
    "        colors = (\"blue\", \"red\", \"green\")\n",
    "        markers = (\"^\", \"s\", \"o\")\n",
    "\n",
    "        for target_class, color, marker in zip(target_classes, colors, markers):\n",
    "            ax1.scatter(\n",
    "                x=X_train_transformed[y_train == target_class, 0],\n",
    "                y=X_train_transformed[y_train == target_class, 1],\n",
    "                color=color,\n",
    "                label=f\"class {target_class}\",\n",
    "                alpha=0.5,\n",
    "                marker=marker,\n",
    "            )\n",
    "\n",
    "            ax2.scatter(\n",
    "                x=X_train_std_transformed[y_train == target_class, 0],\n",
    "                y=X_train_std_transformed[y_train == target_class, 1],\n",
    "                color=color,\n",
    "                label=f\"class {target_class}\",\n",
    "                alpha=0.5,\n",
    "                marker=marker,\n",
    "            )\n",
    "\n",
    "        ax1.set_title(\"Training dataset after PCA\")\n",
    "        ax2.set_title(\"Standardized training dataset after PCA\")\n",
    "\n",
    "        for ax in (ax1, ax2):\n",
    "            ax.set_xlabel(\"1st principal component\")\n",
    "            ax.set_ylabel(\"2nd principal component\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            ax.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_make_pipeline(pca_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a4377",
   "metadata": {},
   "source": [
    "**Question**  \n",
    "Try changing the KNN algorithm with different ones, such as Gaussian Naïve Bayes and Decision Trees in the pipeline. What do you notice in the accuracy of the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f049513",
   "metadata": {},
   "source": [
    "### sneak peek to the future session (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e89e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_make_pipeline(pca_enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25847cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232ba604",
   "metadata": {},
   "source": [
    "## Now we move on the next session which is about categorial features and data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a61ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6977889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the csv file and skim through it. It does not have column names \n",
    "# so we will allocate names to each column \n",
    "\n",
    "# Naming the Columns\n",
    "names = ['age','workclass','fnlwgt','education','education-num',\n",
    "        'marital-status','occupation','relationship','race','sex',\n",
    "        'capital-gain','capital-loss','hours-per-week','native-country',\n",
    "        'y']\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../../data/adult.csv', names=names, na_values='?')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99897885",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Get the unique values in the race column \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Check the unique values in the 'y' column \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ce71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Get the popluation count by race and plot the pie chart of race column\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c51abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see redundant space prefix in the values. Remove them.\n",
    "df['race'] = df['race'].apply(lambda x: x.strip())\n",
    "df['y'] = df['y'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'].unique(), df['y'].unique(), df['occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded9523",
   "metadata": {},
   "source": [
    "Hmmm it's not just the race and y column. Occupation also has redundant space prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to apply this to all the string-valued columns\n",
    "for col_name in df.columns:\n",
    "    if not 'int' in str(df[col_name].dtype):\n",
    "        df[col_name] = df[col_name].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e919df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    # checking only the columns that is not integer type\n",
    "    if not 'int' in str(df[col_name].dtype):\n",
    "        print(df[col_name].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51257e6",
   "metadata": {},
   "source": [
    "All done!  \n",
    "Now let's specifically look into the 'race' and 'y' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['race', 'y']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Convert categorical and target variable (column 'y') to binary numerical values\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f7c0c",
   "metadata": {},
   "source": [
    "Now, let's map the occupation into different numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f09193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For occupation converting different categories to numerical values\n",
    "occ_mapping = {\n",
    "    'Priv-house-serv':0,'?':-1, 'Other-service':0,'Handlers-cleaners':0,\n",
    "    'Farming-fishing':1,'Machine-op-inspct':1,'Adm-clerical':1,\n",
    "    'Transport-moving':2,'Craft-repair':2,'Sales':2,\n",
    "    'Armed-Forces':3,'Tech-support':3,'Protective-serv':3,\n",
    "    'Prof-specialty':4,'Exec-managerial':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: using 'map' function in pandas, map the categorical values into numerical values \n",
    "# and save it to df['occupation']\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897db437",
   "metadata": {},
   "source": [
    "### Dealing with Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25831b63",
   "metadata": {},
   "source": [
    "#### In processing the data earlier, we did not take account of the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0c2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TASK 6\n",
    "# Drop the missing values, i.e. values with '?' in the occupation and native country \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db45aba",
   "metadata": {},
   "source": [
    "Look, the the number of rows shrinked down to 30162 from 32561"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3d8c8",
   "metadata": {},
   "source": [
    "Above is the case where we want specific ordinal values for each occupation. What if we don't care?\n",
    "We can use LabelEncoder\n",
    "###  Basic Usage of LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode workclass column\n",
    "work_class_list = list(df['workclass'].unique())\n",
    "work_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72068258",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(work_class_list)\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d69621",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.transform(work_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548b6ec",
   "metadata": {},
   "source": [
    "### Now let's use this in our df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287425ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54976e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = ['workclass','education','marital-status','relationship', 'sex', 'native-country']\n",
    "\n",
    "# Encoding Categorical Columns\n",
    "# label_encoder.fit_transform fits label encoder and returns encoded labels.\n",
    "df[categ] = df[categ].apply(label_encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d0e0b",
   "metadata": {},
   "source": [
    "#### Question 1 : Try training a classifier with and without dealing with missing values\n",
    "#### Question 2: Try OneHotEncoder instead of LabelEncoder and compare the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aabc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2abcae20776490ba31080de71a1a7d4bc1f223b71e3c8ef1f9c424fbab927df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
